{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rbih-GWb2z7g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn  # for neural network layers\n",
        "import torch.optim as optim  # for optimization algorithms\n",
        "import torch.nn.functional as F  # for activation and loss functions\n",
        "from torch.utils.data import DataLoader  # for batching and loading datasets\n",
        "import torchvision.datasets as datasets  # for standard datasets like MNIST, CIFAR10, etc.\n",
        "import torchvision.transforms as transforms  # for data transformations (normalization, augmentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NN(nn.Module):  # ✅ 'Module' not 'Moulde'\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(NN, self).__init__()  # ✅ correct super() call\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.fc2 = nn.Linear(50, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))   # ✅ apply ReLU after first layer\n",
        "        x = self.fc2(x)           # ✅ last layer usually without activation for classification\n",
        "        return x"
      ],
      "metadata": {
        "id": "Z3GRjbTq5Gpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channel=1, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        # Input: in_channel x 28 x 28 (e.g., grayscale MNIST image)\n",
        "        # Output: 8 x 28 x 28 (padding=1 keeps spatial size same)\n",
        "        # kernel_size=(3,3) scans 3x3 patches, stride=1 moves one pixel at a time\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channel,\n",
        "            out_channels=8,\n",
        "            kernel_size=(3,3),\n",
        "            stride=(1,1),\n",
        "            padding=(1,1)\n",
        "        )\n",
        "\n",
        "        # Max pooling layer\n",
        "        # Reduces each feature map spatially by half\n",
        "        # kernel_size=(2,2), stride=(2,2) → 28x28 becomes 14x14\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "        # Second convolutional layer\n",
        "        # Input: 8 x 14 x 14 (from previous pooling)\n",
        "        # Output: 16 x 14 x 14 (padding=1 keeps spatial size)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=8,\n",
        "            out_channels=16,\n",
        "            kernel_size=(3,3),\n",
        "            stride=(1,1),\n",
        "            padding=(1,1)\n",
        "        )\n",
        "\n",
        "        # Fully connected (linear) layer\n",
        "        # Input: 16 feature maps of size 7x7 → flattened to 16*7*7=784\n",
        "        # Output: num_classes neurons (e.g., 10 for MNIST)\n",
        "        self.fc1 = nn.Linear(16*7*7, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply first convolution + ReLU activation\n",
        "        # ReLU introduces non-linearity\n",
        "        x = F.relu(self.conv1(x))\n",
        "\n",
        "        # Apply max pooling\n",
        "        x = self.pool(x)  # reduces spatial size from 28x28 → 14x14\n",
        "\n",
        "        # Apply second convolution + ReLU activation\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        # Apply second max pooling\n",
        "        x = self.pool(x)  # reduces spatial size from 14x14 → 7x7\n",
        "\n",
        "        # Flatten the 16x7x7 feature maps into a single vector per example\n",
        "        x = torch.flatten(x, 1)  # shape: [batch_size, 16*7*7]\n",
        "\n",
        "        # Fully connected layer: output logits for each class\n",
        "        x = self.fc1(x)  # shape: [batch_size, num_classes]\n",
        "\n",
        "        # Return raw logits (will be used with CrossEntropyLoss)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "XEZCXNreuBfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=CNN()\n",
        "x=torch.randn(64,1,28,28)\n",
        "print(model(x).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S8VlviL_4y1",
        "outputId": "3878a4be-bb8b-422e-eac5-377294773a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper parameters\n",
        "in_channel=1\n",
        "num_classes=10\n",
        "learning_rate=0.001\n",
        "batch_size= 64\n",
        "num_epochs= 1"
      ],
      "metadata": {
        "id": "pQZ_HYUZAruP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "tqf9TA9rA0I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data loading\n",
        "train_dataset=datasets.MNIST(root='dataset/',train=True, transform=transforms.ToTensor(),download=True)\n",
        "train_loader= DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_dataset= datasets.MNIST(root='dataset/',train=False, transform=transforms.ToTensor(),download=True)\n",
        "test_loader= DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "WvcoJl4eAyhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "model=CNN().to(device)"
      ],
      "metadata": {
        "id": "3wMfH5LVA4rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss and optimizer\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "cmttr_dGBfsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Move data & targets to device (CPU or GPU)\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # Forward pass\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()  # ✅ correct method is zero_grad(), not zerograd()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "qyx1dX8nBcIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # evaluation mode (disables dropout/batchnorm)\n",
        "\n",
        "    with torch.no_grad():  # no gradient computation needed\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)  # predicted class\n",
        "\n",
        "            num_correct += (predictions == y).sum().item()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    acc = float(num_correct) / float(num_samples)\n",
        "    print(f'Got {num_correct}/{num_samples} with accuracy {acc*100:.2f}%')\n",
        "\n",
        "    model.train()  # back to training mode\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "fkOO8XX5BzpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_accuracy(train_loader,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyN842r3B4Ck",
        "outputId": "fd7b2160-c8e4-47ce-a588-968c8f219f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 57894/60000 with accuracy 96.49%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9649"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_accuracy(test_loader,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuJYBktVB-Di",
        "outputId": "8dabfb8a-853a-4d58-bb3a-0f5ff63a5e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 57894/60000 with accuracy 96.49%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9649"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    check_accuracy(test_loader,model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxbMYCqTDRFM",
        "outputId": "29e14205-391e-4453-d8da-1b4b5ddd34a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 57894/60000 with accuracy 96.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN **model**"
      ],
      "metadata": {
        "id": "23dmQCNO88Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "xxllmbCG9eae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper parameters\n",
        "input_size=784\n",
        "num_classes=10\n",
        "learning_rate=0.001\n",
        "batch_size= 64\n",
        "num_epochs= 1"
      ],
      "metadata": {
        "id": "oheiFxTq-C7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data loading\n",
        "train_dataset=datasets.MNIST(root='dataset/',train=True, transform=transforms.ToTensor(),download=True)\n",
        "train_loader= DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_dataset= datasets.MNIST(root='dataset/',train=False, transform=transforms.ToTensor(),download=True)\n",
        "test_loader= DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "26NHaqO4EehI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "model=NN(input_size=input_size,num_classes=num_classes).to(device)"
      ],
      "metadata": {
        "id": "f6PFF85sNmv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss and optimizer\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "bov1HjdjN_4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Move data & targets to device (CPU or GPU)\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # Flatten images for fully connected network\n",
        "        data = data.reshape(data.shape[0], -1)  # or data.view(data.shape[0], -1)\n",
        "\n",
        "        # Forward pass\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()  # ✅ correct method is zero_grad(), not zerograd()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9RFu9wWZhIqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # evaluation mode (disables dropout/batchnorm)\n",
        "\n",
        "    with torch.no_grad():  # no gradient computation needed\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "            x = x.reshape(x.shape[0], -1)  # flatten images\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)  # predicted class\n",
        "\n",
        "            num_correct += (predictions == y).sum().item()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    acc = float(num_correct) / float(num_samples)\n",
        "    print(f'Got {num_correct}/{num_samples} with accuracy {acc*100:.2f}%')\n",
        "\n",
        "    model.train()  # back to training mode\n",
        "    return acc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N91VkDKuk9Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_accuracy(train_loader,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNwk2CM7otL4",
        "outputId": "cfc07809-9671-45b3-d75f-2824c446ca89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 55858/60000 with accuracy 93.10%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9309666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_accuracy(test_loader,model)"
      ],
      "metadata": {
        "id": "jM1CotHko0XD",
        "outputId": "c8678553-7739-4198-d892-cbc14710e03d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 55858/60000 with accuracy 93.10%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9309666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}